import numpy as np # linear algebra
from tensorflow.keras.applications import VGG16
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import Model
from tensorflow.keras.callbacks import Callback
import os
from tensorflow.keras.utils import to_categorical
from tqdm import tqdm
from sklearn.utils import shuffle
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, plot_confusion_matrix
import seaborn as sns


SIZE = 112 #224 recomended as it is default for VGG16

train_images = np.load('Train_Data/images_train.npy') #Loading Data Generated by test_train_generator.py
train_labels = np.load('Train_Data/labels_train.npy')
print('Training Data Loaded Successfully...')
train_images,train_labels = shuffle(train_images,train_labels)

''' Initializing the VGG16 with imagenet pretrained weights, with out including top 3 layers,
Addition of custom layers and All layers of VGG16 is set to trainable'''

class myCallback(Callback):
    def on_epoch_end(self, epoch, logs={}):
        if(logs.get('accuracy') is not None and logs.get('accuracy')>=0.95):
          print("\nReached 95% accuracy so cancelling training!")
          self.model.stop_training = True

callbacks = myCallback()

model = VGG16(input_shape=(SIZE, SIZE, 3),weights='imagenet', include_top=False)
for layer in model.layers:
    layer.trainable=True #setting VGG Layers to trainable
output = model.output
output = Flatten()(output)
output = Dense(500,activation='relu')(output)
output = Dense(50,activation='relu')(output)
finallayer = Dense(4, activation='softmax')(output) #Here the number of output classes are 4, initialize with yours

updated_model = Model(inputs = model.input, outputs=finallayer)

updated_model.compile(optimizer = Adam(lr = 1e-4), loss = 'categorical_crossentropy', metrics = ['accuracy'])

history = updated_model.fit(train_images,train_labels, validation_split=0.1,epochs=50, batch_size=50,verbose=2, shuffle=True,callbacks=[callbacks])


'''Plotting Training History'''

print(history.history.keys())
plt.subplot(211)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])  # If you feed validation data / split data for validation
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
# plt.show()
# summarize history for loss
plt.subplot(212)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])  # If you feed validation data / split data for validation
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
print('Press any key to continue...')
plt.waitforbuttonpress(timeout=- 1)
plt.close()

del train_images
del train_labels
print('Training has finished...')


'''Testing the model and producing classification report, confusion matrix'''

test_images = np.load('Test_Data/images_test.npy')
test_labels = np.load('Test_Data/labels_test.npy')
print('Test Data Loaded Successfully...')
predicted_labels = updated_model.predict(test_images).argmax(axis=1)
test_labels = test_labels.argmax(axis=1)

print(accuracy_score(test_labels, predicted_labels))
cm=confusion_matrix(test_labels, predicted_labels)
print(classification_report(test_labels, predicted_labels))
#print(confusion_matrix(test_labels.argmax(axis=1), y_pred.argmax(axis=1)))

ax= plt.subplot()
sns.heatmap(cm, fmt='g',annot=True, ax = ax,cmap=plt.cm.Blues) #annot=True to annotate cells

# labels, title and ticks
ax.set_xlabel('Predicted labels')
ax.set_ylabel('True labels')
ax.set_title('Confusion Matrix')
ax.xaxis.set_ticklabels(['Codewalk','Handwritten','Misc','Slides'])
ax.yaxis.set_ticklabels(['Codewalk','Handwritten','Misc','Slides'])

plt.show()


'''Saving the model'''

model.save('Model/VGG16_DocClassifier.h5')

print("Model is saved in Model directory...")
